[supervisord]
nodaemon=true
logfile=/var/log/promptctl/supervisord.log
pidfile=/var/run/supervisord.pid
childlogdir=/var/log/promptctl
user=root

[unix_http_server]
file=/var/run/supervisor.sock
chmod=0700

[rpcinterface:supervisor]
supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface

[supervisorctl]
serverurl=unix:///var/run/supervisor.sock

[program:ollama]
command=/bin/ollama serve
autostart=true
autorestart=true
priority=10
stdout_logfile=/var/log/promptctl/ollama.log
stderr_logfile=/var/log/promptctl/ollama-error.log
environment=OLLAMA_HOST="0.0.0.0:11434",OLLAMA_MODELS="/root/.ollama/models"
startsecs=5
startretries=3

[program:promptctl-daemon]
command=/usr/bin/python3 /app/promptctl.py --repo /home/promptctl/.promptctl daemon --socket --socket-port 9090 --interval 60 --conflict-strategy timestamp --auto-optimize
directory=/app
user=promptctl
autostart=true
autorestart=true
priority=20
stdout_logfile=/var/log/promptctl/daemon.log
stderr_logfile=/var/log/promptctl/daemon-error.log
environment=PYTHONUNBUFFERED="1",PROMPTCTL_REPO="/home/promptctl/.promptctl"
startsecs=10
startretries=3

[program:model-loader]
command=/bin/bash -c "sleep 15 && /bin/ollama pull phi3.5 2>/dev/null || echo 'Model pull skipped or failed'"
autostart=true
autorestart=false
priority=30
stdout_logfile=/var/log/promptctl/model-loader.log
stderr_logfile=/var/log/promptctl/model-loader-error.log
startsecs=0
exitcodes=0,1
